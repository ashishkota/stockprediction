{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset used - Stock_train.csv which tackles the classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, log_loss\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "import itertools\n",
    "\n",
    "import tensorflow\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "\n",
    "import bs4 as bs\n",
    "import requests\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in the csv file\n",
    "data = pd.read_csv(r'D:\\Stock_train.csv', sep = \",\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the percentage of NaN's in each column\n",
    "percent_missing = round((data.isnull().sum() * 100) / len(data),2)\n",
    "missing_col_percent = pd.DataFrame({'column_name': data.columns, 'percent_missing': percent_missing})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_col_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding all the columns with more than 50% of Nans\n",
    "more_than_fifty = []\n",
    "for index,row in missing_col_percent.iterrows():\n",
    "    if row['percent_missing'] > 50:\n",
    "        more_than_fifty.append(row['column_name'])\n",
    "more_than_fifty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the columns with more than 50% Nans\n",
    "data = data.drop(more_than_fifty, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing out all the columns with atleast 1 Nan\n",
    "nan_cols = [i for i in data.columns if data[i].isnull().any()]\n",
    "nan_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Columns with atleast 1 Nan\n",
    "len(nan_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling all the Nans with median of each sector's respective column \n",
    "data = data.fillna(data.groupby('Sector').transform('median'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning the stock tickers to a new list\n",
    "name_stocks = data['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seperating the categorical and numerical columns\n",
    "categorical_values = data[list(data.select_dtypes(include='object').columns)]\n",
    "numerical_values = data[list(data.select_dtypes(include=['float64', 'int64']).columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the X and Y datasets and assigning all the numerical columns to X while dropping the class column which is our y column\n",
    "X = numerical_values.drop(['Class'] , axis = 1)\n",
    "y = numerical_values['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since sector is a categorical variable we need to get_dummies. \n",
    "X['Sector'] = categorical_values['Sector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dummy variables for Sector variable\n",
    "dummies = pd.get_dummies(X['Sector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenating the dummes back into out X dataset\n",
    "X = pd.concat([X, dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the sector varible\n",
    "X = X.drop(['Sector'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#co-relation data\n",
    "data.corr()['Class'].abs().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping operating margin column as it has no correlation with out target variable\n",
    "X = X.drop(['operatingProfitMargin'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling our variable to get accurate results\n",
    "std_scal = StandardScaler()\n",
    "X_std = std_scal.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cor-relation amongst each other\n",
    "X.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting into train,test and validation datasets\n",
    "X_train_all, X_test, y_train_all, y_test = train_test_split(X_std, y, test_size = 0.05, random_state = 90)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train_all, y_train_all, test_size = 0.20, random_state = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Functions\n",
    "\n",
    "#Function to calculate performance results\n",
    "def performance_results(model, X, y): \n",
    "    # Checks the accuracy on the validation sample\n",
    "    y_hat = model.predict(X)\n",
    "\n",
    "    #metrics\n",
    "    conf_mat = confusion_matrix(y, y_hat)\n",
    "    acc_score = accuracy_score(y, y_hat)\n",
    "    log_l = log_loss(y, y_hat)\n",
    "\n",
    "    #graph\n",
    "    plt.figure(figsize=(10,7))\n",
    "    sns.heatmap(conf_mat, annot=True, cmap = 'hot_r')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Truth')\n",
    "    return acc_score, log_l, y_hat\n",
    "\n",
    "def graph_cv(cv_res):\n",
    "    # Let's plot the value of the cross-validation to see how the Mean squred error change with alpha\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(cv_res['param_C'], cv_res['mean_train_score'])\n",
    "    plt.plot(cv_res['param_C'], cv_res['mean_test_score'])\n",
    "    plt.xlabel('C')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(\"Accuracy with penalization\")\n",
    "    plt.legend(['train accuracy', 'test accuracy'], loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \n",
    "    #This function prints and plots the confusion matrix..\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 - Logistic Regression - Ridge Penalization in Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Penalization in classification - Lasso-Ridge\n",
    "lr_ridge = LogisticRegression(solver = \"saga\", tol = 0.001,\n",
    "                                   penalty = 'l2', random_state = 90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrating our shrink parameter 'C'\n",
    "params ={'C':np.linspace(0.0001,0.1, 10)}\n",
    "lr_ridge_cv = GridSearchCV(estimator = lr_ridge, \n",
    "                                    param_grid = params,\n",
    "                                    scoring =\"neg_log_loss\",\n",
    "                                    return_train_score = True,\n",
    "                                    cv =10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting our test and train sets\n",
    "lr_ridge_cv.fit(X_train, y_train)\n",
    "cv_results = pd.DataFrame(lr_ridge_cv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the value of the cross-validation to see how the Mean squred error change with alpha\n",
    "graph_cv(cv_results)\n",
    "# this is the value of the best lambda\n",
    "print(\"Best penalization parameter (C): \", lr_ridge_cv.best_params_['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Results\n",
    "r_acc_score, r_log_l, r_y_hat_validation = performance_results(lr_ridge_cv, X_validation, y_validation)\n",
    "print(\"validation acc_score       : \", r_acc_score)\n",
    "print(\"validation log loss      : \", r_log_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparision of real and predicted values \n",
    "name_stocks.iloc[y_validation.index,]\n",
    "output_df = pd.DataFrame(np.hstack([y_validation.values.reshape(-1,1), r_y_hat_validation.reshape(-1,1)]), \n",
    "                        index = list(name_stocks.iloc[y_validation.index,]), \n",
    "                        columns = ['Real', 'Predicted'])\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Results\n",
    "rt_acc_score, rt_log_l, rt_y_hat_test_ridge = performance_results(lr_ridge_cv, X_test, y_test)\n",
    "print(\"test acc_score       : \", rt_acc_score)\n",
    "print(\"test log loss      : \", rt_log_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using stock tickers to see the relative data\n",
    "name_of_stock = list(name_stocks.iloc[y_test.index,])\n",
    "print(\"Names stock: \", name_of_stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df_test = pd.DataFrame(np.hstack([y_test.values.reshape(-1,1), rt_y_hat_test_ridge.reshape(-1,1)]), \n",
    "                        index = name_of_stock, \n",
    "                        columns = ['Real', 'Predicted Ridge'])\n",
    "output_df_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 - Logistic Regression - Lasso Penalization for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our penalized Logistic regression using Lasso penalty\n",
    "lr_lasso = LogisticRegression(solver = \"saga\", tol = 0.001,\n",
    "                                   penalty = 'l1', random_state = 90, class_weight = 'None')\n",
    "                                   # class_weight = 'balanced'\n",
    "\n",
    "\n",
    "# Calibrating our shrink parameter 'C'\n",
    "params ={'C':np.linspace(0.001,1, 10)}\n",
    "lr_lasso_cv = GridSearchCV(estimator = lr_lasso, \n",
    "                                    param_grid = params,\n",
    "                                    scoring =\"neg_log_loss\",\n",
    "                                    return_train_score = True,\n",
    "                                    cv =10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "lr_lasso_cv.fit(X_train, y_train)\n",
    "cv_results = pd.DataFrame(lr_lasso_cv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the value of the cross-validation to see how the Mean squred error change with alpha\n",
    "graph_cv(cv_results)\n",
    "# this is the value of the best lambda\n",
    "print(\"Best penalization parameter (C): \", lr_lasso_cv.best_params_['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Results\n",
    "l_acc_score, l_log_l, l_y_hat_val_lasso = performance_results(lr_lasso_cv, X_validation, y_validation)\n",
    "print(\"validation acc_score       : \", l_acc_score)\n",
    "print(\"validation log loss      : \", l_log_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Results\n",
    "lt_acc_score, lt_log_l, lt_y_hat_test_lasso = performance_results(lr_lasso_cv, X_test, y_test)\n",
    "print(\"test acc_score       : \", lt_acc_score)\n",
    "print(\"test log loss      : \", lt_log_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparision of predicted and reak outputs \n",
    "output_df = pd.DataFrame(np.hstack([y_test.values.reshape(-1,1), lt_y_hat_test_lasso.reshape(-1,1), rt_y_hat_test_ridge.reshape(-1,1)]), \n",
    "                        index = name_of_stock, \n",
    "                        columns = ['Real', 'Predicted Lasso', 'Predicted Ridge'])\n",
    "output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output of lasso and ridge predicted probabilities \n",
    "lasso_pred_proba = lr_lasso_cv.predict_proba(X_test)\n",
    "ridge_pred_proba = lr_ridge_cv.predict_proba(X_test)\n",
    "\n",
    "lasso_pred_proba_df = pd.DataFrame(np.hstack([lasso_pred_proba, lt_y_hat_test_lasso.reshape(-1,1), y_test.values.reshape(-1,1)]),\n",
    "                    index = name_of_stock, \n",
    "                    columns = ['Prob 0', 'Prob 1', 'Lasso pred', 'True'])\n",
    "\n",
    "ridge_pred_proba_df = pd.DataFrame(np.hstack([ridge_pred_proba, rt_y_hat_test_ridge.reshape(-1,1), y_test.values.reshape(-1,1)]),\n",
    "                    index = name_of_stock, \n",
    "                    columns = ['Prob 0', 'Prob 1', 'Ridge pred', 'True'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_pred_proba_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_pred_proba_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3 - Random Forest in Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest \n",
    "rf = RandomForestClassifier(random_state = 90)\n",
    "\n",
    "parameters = {'n_estimators':[2,5,10,15,40,50,80,100,150,200],\n",
    "             'max_depth':range(1,5)}\n",
    "\n",
    "rf_cv =GridSearchCV(estimator = rf, \n",
    "                    param_grid = parameters,\n",
    "                    scoring =\"neg_log_loss\",  \n",
    "                    return_train_score = True,\n",
    "                    cv =10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the model\n",
    "rf_cv.fit(X_train, y_train)\n",
    "print(rf_cv.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation results\n",
    "rf_acc_score, rf_log_l, rf_y_hat_validation = performance_results(rf_cv, X_validation, y_validation)\n",
    "print(\"validation acc_score       : \", rf_acc_score)\n",
    "print(\"validation log loss      : \", rf_log_l)\n",
    "\n",
    "# Test results\n",
    "rft_acc_score, rft_log_l, rft_y_hat_test = performance_results(rf_cv, X_test, y_test)\n",
    "print(\"test acc_score       : \", rft_acc_score)\n",
    "print(\"test log loss      : \", rft_log_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of real and predicted data\n",
    "output_df = pd.DataFrame(np.hstack([y_test.values.reshape(-1,1), rft_y_hat_test.reshape(-1,1)]), \n",
    "                        index = list(name_stocks.iloc[y_test.index].values), \n",
    "                        columns = ['Real', 'Random Forest Predicted'])\n",
    "output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4 - Random Forest Classification with ADA Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Boosting\n",
    "# ADA Boosting Classifier\n",
    "dtc = DecisionTreeClassifier(max_depth =3 ,random_state = 90)\n",
    "adab_clf = AdaBoostClassifier(base_estimator = dtc, random_state = 90)\n",
    "\n",
    "parameters = {'n_estimators':[2,5,10],\n",
    "             'learning_rate':np.linspace(1,5,4)}\n",
    "\n",
    "# Specify the cross-validation\n",
    "adab_clf_cv =GridSearchCV(estimator = adab_clf, \n",
    "                    param_grid = parameters,\n",
    "                    scoring =\"neg_log_loss\",  # accuracy\n",
    "                    return_train_score = True,\n",
    "                    cv =10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adab_clf_cv.fit(X_train, y_train)\n",
    "print(adab_clf_cv.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation results\n",
    "ada_acc_score, ada_log_l, ada_y_hat_validation = performance_results(adab_clf_cv, X_validation, y_validation)\n",
    "print(\"validation acc_score       : \", ada_acc_score)\n",
    "print(\"validation log loss      : \", ada_log_l)\n",
    "\n",
    "#Test results\n",
    "adat_acc_score, adat_log_l, adat_y_hat_test = performance_results(adab_clf_cv, X_test, y_test)\n",
    "print(\"test acc_score       : \", adat_acc_score)\n",
    "print(\"test log loss      : \", adat_log_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 5 - Random Forest Classification with Gradient Boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Classifier \n",
    "\n",
    "gb_clf = GradientBoostingClassifier(random_state = 90)\n",
    "\n",
    "parameters = {'n_estimators':[2,5,10],\n",
    "             'learning_rate':np.linspace(0.1,0.5,5)}\n",
    "\n",
    "# Specify the cross-validation\n",
    "gb_clf_cv =GridSearchCV(estimator = gb_clf, \n",
    "                    param_grid = parameters,\n",
    "                    scoring =\"neg_log_loss\",  # accuracy\n",
    "                    return_train_score = True,\n",
    "                    cv =10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the model \n",
    "gb_clf_cv.fit(X_train, y_train)\n",
    "print(gb_clf_cv.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation results\n",
    "gb_acc_score, gb_log_l, gb_y_hat_validation = performance_results(gb_clf_cv, X_validation, y_validation)\n",
    "print(\"validation acc_score       : \", gb_acc_score)\n",
    "print(\"validation log loss      : \", gb_log_l)\n",
    "\n",
    "# Test results\n",
    "gbt_acc_score, gbt_log_l, gbt_y_hat_test = performance_results(gb_clf_cv, X_test, y_test)\n",
    "print(\"test acc_score       : \", gbt_acc_score)\n",
    "print(\"test log loss      : \", gbt_log_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 6 - Neural Networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Networks\n",
    "y_train_nn = np.array(pd.get_dummies(y_train).values)\n",
    "y_test_nn = np.array(pd.get_dummies(y_test).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a model\n",
    "def classification_model(x, y, n_neurons):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_neurons[0], input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(y.shape[1], kernel_initializer='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons = [80]\n",
    "classification_model_hl = classification_model(X_train, y_train_nn, n_neurons)\n",
    "classification_model_hl.summary()\n",
    "\n",
    "# Fit the model\n",
    "classification_model_hl.fit(X_train, y_train_nn, epochs=100, batch_size=5, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = classification_model_hl.evaluate(X_test, y_test_nn, verbose=0)\n",
    "print('Test log-loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classification_model_hl.predict(X_test)\n",
    "# Convert predictions probabilities to classes\n",
    "# We need select which classes has the highest probability\n",
    "y_pred_classes = np.argmax(y_pred, axis = 1) \n",
    "\n",
    "# Compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(y_test, y_pred_classes) \n",
    "\n",
    "\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes = range(y_pred.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating lists of all the accuracy scores and log losses for each model \n",
    "acc_scores = []\n",
    "log_loss = []\n",
    "lables = []\n",
    "lables.extend(['Lasso', 'Ridge', 'Random Forest', 'ADA Booster', 'Gb' , 'Neural Networks' ])\n",
    "acc_scores.extend([lt_acc_score, rt_acc_score, rft_acc_score, adat_acc_score, gbt_acc_score, scores[1]])\n",
    "log_loss.extend([lt_log_l, rt_log_l, rft_log_l, adat_log_l, gbt_log_l, scores[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Dataframe of the comparisions \n",
    "scores = {'Lables': lables,\n",
    "          'Accuracy Scores': acc_scores,\n",
    "           'Log Loss': log_loss}\n",
    "\n",
    "scores_df = pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Booster is the best model for this data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading stock_test dataset\n",
    "test_data = pd.read_csv(r'D:\\Stock_test.csv', sep = \",\")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns that we dint use for training the model\n",
    "test_data = test_data.drop(more_than_fifty, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing all the missing value with medians grouped by each sector\n",
    "test_data = test_data.fillna(data.groupby('Sector').transform('median'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the test stock tickers \n",
    "test_name_stocks = test_data['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.drop(['Unnamed: 0', 'operatingProfitMargin'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dummies\n",
    "dummies_test = pd.get_dummies(test_data['Sector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.concat([test_data, dummies_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.drop(['Sector'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the test data\n",
    "std_scal = StandardScaler()\n",
    "test_data_std = std_scal.fit_transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since gradient boosting gave us the vest results, we will use that to predict our class for the test set\n",
    "y_hat_final = gb_clf_cv.predict(test_data_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the results to dataframe\n",
    "y_df = pd.DataFrame(y_hat_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df [\"Stock tickers\"] = test_name_stocks\n",
    "y_df = y_df[[\"Stock tickers\", 0]]\n",
    "y_df = y_df.rename(columns={0: \"Result\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe with all the stockes that are profitable according to our model\n",
    "y_df_ones = y_df[y_df[\"Result\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df, y_df_ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading the actual 2018 and 2019 prices of our stocks \n",
    "one_tickers = list(y_df_ones['Stock tickers'])\n",
    "Tickers = one_tickers\n",
    "start = datetime.datetime(2018,12,31)\n",
    "end = datetime.datetime(2019,12,31)\n",
    "data = yf.download(Tickers, start = start, end = end, interval = '1d')\n",
    "prices = data['Adj Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the columns with no 2018 data \n",
    "prices = prices.dropna(axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_2018 = prices.iloc[0,:] #2018 price\n",
    "price_2019 = prices.iloc[-1,:] #2019 price\n",
    "\n",
    "series = { '2018 price': price_2018 , '2019 price': price_2019 } #series of the prices\n",
    "\n",
    "prices_df = pd.DataFrame(series) # dataframe of the prices\n",
    "\n",
    "prices_df['Price Change'] = prices_df['2019 price'] - prices_df['2018 price'] #computing the difference of the 2 prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing the absolute variation. If we buy 1 unit of each sof the stock, the net change in the portfolio is avsolute variation\n",
    "absolute_variation = prices_df['Price Change'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_variation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
